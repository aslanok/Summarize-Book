{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aslan/Documents/dataScience/rag_langchain/myenv/lib/python3.9/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "/Users/aslan/Documents/dataScience/rag_langchain/myenv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import markdown2\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('alice_in_wonderland.md', 'r', encoding='utf-8') as file:\n",
    "    markdown_content = file.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_content = markdown2.markdown(markdown_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap the text in a Document object\n",
    "documents = [Document(page_content=text_content, metadata={\"source\": \"alice_in_wonderland.md\"})]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text(documents: list[Document]):\n",
    "    # Initialize the text splitter\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=400,           # Maximum size of each chunk\n",
    "        chunk_overlap=50,         # Overlap between chunks\n",
    "        length_function=len,       # Function to determine the length of the chunk\n",
    "        add_start_index=True,      # Include the starting index in the metadata\n",
    "    )\n",
    "\n",
    "    # Split the documents into chunks\n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "    print(f\"Split {len(documents)} documents into {len(chunks)} chunks.\")\n",
    "\n",
    "    # Example: Inspect the content and metadata of the 10th chunk\n",
    "    document = chunks[10]\n",
    "    print(document.page_content)  # Print the chunk content\n",
    "    print(document.metadata)      # Print the metadata\n",
    "\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 1 documents into 587 chunks.\n",
      "watch out of its waistcoat-pocket</em>, and looked at it, and then hurried\n",
      "on, Alice started to her feet, for it flashed across her mind that she\n",
      "had never before seen a rabbit with either a waistcoat-pocket, or a\n",
      "watch to take out of it, and burning with curiosity, she ran across the\n",
      "field after it, and fortunately was just in time to see it pop down a\n",
      "large rabbit-hole under the hedge.</p>\n",
      "{'source': 'alice_in_wonderland.md', 'start_index': 2450}\n"
     ]
    }
   ],
   "source": [
    "chunks = split_text(documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddingModel = SentenceTransformer('all-MiniLM-L6-v2')  # You can choose another model as well\n",
    "\n",
    "# Embed the text chunks\n",
    "chunk_embeddings = []\n",
    "for chunk in chunks:\n",
    "    embedding = embeddingModel.encode(chunk.page_content)\n",
    "    chunk_embeddings.append({\n",
    "        \"embedding\": embedding,\n",
    "        \"metadata\": chunk.metadata,\n",
    "        \"content\": chunk.page_content\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chromadb import Client\n",
    "from chromadb.config import Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from uuid import uuid4  # Import uuid4 to generate unique IDs\n",
    "\n",
    "# Initialize ChromaDB client\n",
    "client = Client(Settings(persist_directory='./chromadb_data'))\n",
    "\n",
    "# Create a collection in ChromaDB\n",
    "#collection = client.create_collection('story_chunks')\n",
    "\n",
    "\n",
    "# Connect to the existing collection\n",
    "collection = client.get_collection('story_chunks')\n",
    "\n",
    "# Add embeddings and corresponding metadata to the database\n",
    "for chunk_data in chunk_embeddings:\n",
    "    # Generate a unique ID for each document\n",
    "    doc_id = str(uuid4())\n",
    "    \n",
    "    # Convert the numpy array to a list\n",
    "    embedding_list = chunk_data[\"embedding\"].tolist()\n",
    "\n",
    "    collection.add(\n",
    "        ids=[doc_id],                      # Add the unique ID here\n",
    "        documents=[chunk_data[\"content\"]],\n",
    "        embeddings=[embedding_list],       # Ensure embedding is in list format\n",
    "        metadatas=[chunk_data[\"metadata\"]]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query results:\n",
      "Result 1:\n",
      "Content: ['older than you, and must know better;‚Äù and this Alice would not allow\\nwithout knowing how old it was, and, as the Lory positively refused to\\ntell its age, there was no more to be said.</p>', '<p>Just at this moment Alice felt a very curious sensation, which puzzled\\nher a good deal until she made out what it was: she was beginning to\\ngrow larger again, and she thought at first she would get up and leave\\nthe court; but on second thoughts she decided to remain where she was\\nas long as there was room for her.</p>']\n",
      "Metadata: [{'source': 'alice_in_wonderland.md', 'start_index': 24980}, {'source': 'alice_in_wonderland.md', 'start_index': 134140}]\n",
      "ID: ['a2e0fc1c-dc31-46dc-bc14-36c7c03faa31', '07458721-6602-4fe3-80f7-f2685b16a012']\n"
     ]
    }
   ],
   "source": [
    "# Example query\n",
    "query_text = \"Describe Alice's age and appearance in the story.\"\n",
    "query_embedding = embeddingModel.encode(query_text).tolist()\n",
    "\n",
    "# Perform the query\n",
    "query_results = collection.query(query_embeddings=[query_embedding], n_results=2)\n",
    "\n",
    "# Print the results\n",
    "print(\"Query results:\")\n",
    "for i, result in enumerate(query_results[\"documents\"]):\n",
    "    print(f\"Result {i+1}:\")\n",
    "    print(\"Content:\", result)\n",
    "    print(\"Metadata:\", query_results[\"metadatas\"][i])\n",
    "    print(\"ID:\", query_results[\"ids\"][i])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "# Define llm\n",
    "llm = Ollama(model=\"gemma2:2b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Adjusting the Prompt Template\n",
    "prompt = \"\"\"\n",
    "1. Use the following context to answer the question at the end.\n",
    "2. Be precise and avoid speculation. If the information isn't clear, say \"I don't know.\"\n",
    "3. Provide a concise, 2-3 sentence answer.\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Accurate Answer:\"\"\"\n",
    "\n",
    "\n",
    "QA_CHAIN_PROMPT = PromptTemplate.from_template(prompt)\n",
    "\n",
    "llm_chain = LLMChain(\n",
    "    llm=llm, \n",
    "    prompt=QA_CHAIN_PROMPT, \n",
    "    callbacks=None, \n",
    "    verbose=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_prompt = PromptTemplate(\n",
    "    input_variables=[\"page_content\", \"source\"],\n",
    "    template=\"Context:\\ncontent:{page_content}\\nsource:{source}\",\n",
    ")\n",
    "\n",
    "combine_documents_chain = StuffDocumentsChain(\n",
    "    llm_chain=llm_chain,\n",
    "    document_variable_name=\"context\",\n",
    "    document_prompt=document_prompt,\n",
    "    callbacks=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aslan/Documents/dataScience/rag_langchain/myenv/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "embedding_model = HuggingFaceEmbeddings(model_name='all-MiniLM-L6-v2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "vector_store = Chroma.from_documents(\n",
    "    documents=chunks,  # This should be your list of Document objects\n",
    "    embedding=embedding_model,\n",
    "    persist_directory='./chromadb_data',\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain = RetrievalQA.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=vector_store.as_retriever(),  # Use the retriever from the vector store\n",
    "    verbose=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Generated Answer:\n",
      "Alice, a young girl, finds herself falling down a rabbit hole into a whimsical and fantastical world.  The story is full of absurd characters like talking animals and nonsensical situations. \n",
      "\n",
      "She encounters several peculiar events, including a Caucus-race with talking rabbits, a mad tea party where everyone's behavior is unpredictable, and a croquet game played by the Queen who rules over this strange land. Alice even meets an advice-giving Caterpillar! Throughout her journey, she faces challenges like being chased down a path, witnessing the loss of tarts in the hands of someone unknown,  and ultimately tries to unravel a mystery when a mysterious culprit steals them. \n",
      "\n",
      "The story uses symbolism and imagery to explore themes such as childhood imagination, the power of memory, and the absurdity of reality. It's filled with moments that defy logic and question our understanding of time and space. The tale is both entertaining and thought-provoking, leaving readers with a sense of wonder and a desire for further exploration. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"Can you give me a summary of that story. Please give me some details and it should be a little long\"\n",
    "\n",
    "# Run the query through the chain\n",
    "result = qa_chain.run(query)\n",
    "\n",
    "print(\"Generated Answer:\")\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
