{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aslan/Documents/dataScience/rag_langchain/myenv/lib/python3.9/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "/Users/aslan/Documents/dataScience/rag_langchain/myenv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import markdown2\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('alice_in_wonderland.md', 'r', encoding='utf-8') as file:\n",
    "    markdown_content = file.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_content = markdown2.markdown(markdown_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap the text in a Document object\n",
    "documents = [Document(page_content=text_content, metadata={\"source\": \"alice_in_wonderland.md\"})]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text(documents: list[Document]):\n",
    "    # Initialize the text splitter\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=500,           # Maximum size of each chunk\n",
    "        chunk_overlap=50,         # Overlap between chunks\n",
    "        length_function=len,       # Function to determine the length of the chunk\n",
    "        add_start_index=True,      # Include the starting index in the metadata\n",
    "    )\n",
    "\n",
    "    # Split the documents into chunks\n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "    print(f\"Split {len(documents)} documents into {len(chunks)} chunks.\")\n",
    "\n",
    "    # Example: Inspect the content and metadata of the 10th chunk\n",
    "    document = chunks[10]\n",
    "    print(document.page_content)  # Print the chunk content\n",
    "    print(document.metadata)      # Print the metadata\n",
    "\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 1 documents into 464 chunks.\n",
      "passed; it was labelled “ORANGE MARMALADE”, but to her great\n",
      "disappointment it was empty: she did not like to drop the jar for fear\n",
      "of killing somebody underneath, so managed to put it into one of the\n",
      "cupboards as she fell past it.</p>\n",
      "{'source': 'alice_in_wonderland.md', 'start_index': 3679}\n"
     ]
    }
   ],
   "source": [
    "chunks = split_text(documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddingModel = SentenceTransformer('all-MiniLM-L6-v2')  # You can choose another model as well\n",
    "\n",
    "# Embed the text chunks\n",
    "chunk_embeddings = []\n",
    "for chunk in chunks:\n",
    "    embedding = embeddingModel.encode(chunk.page_content)\n",
    "    chunk_embeddings.append({\n",
    "        \"embedding\": embedding,\n",
    "        \"metadata\": chunk.metadata,\n",
    "        \"content\": chunk.page_content\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chromadb import Client\n",
    "from chromadb.config import Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from uuid import uuid4  # Import uuid4 to generate unique IDs\n",
    "\n",
    "# Initialize ChromaDB client\n",
    "client = Client(Settings(persist_directory='./chromadb_data'))\n",
    "\n",
    "# Create a collection in ChromaDB\n",
    "collection = client.create_collection('story_chunks')\n",
    "\n",
    "\n",
    "# Connect to the existing collection\n",
    "collection = client.get_collection('story_chunks')\n",
    "\n",
    "# Add embeddings and corresponding metadata to the database\n",
    "for chunk_data in chunk_embeddings:\n",
    "    # Generate a unique ID for each document\n",
    "    doc_id = str(uuid4())\n",
    "    \n",
    "    # Convert the numpy array to a list\n",
    "    embedding_list = chunk_data[\"embedding\"].tolist()\n",
    "\n",
    "    collection.add(\n",
    "        ids=[doc_id],                      # Add the unique ID here\n",
    "        documents=[chunk_data[\"content\"]],\n",
    "        embeddings=[embedding_list],       # Ensure embedding is in list format\n",
    "        metadatas=[chunk_data[\"metadata\"]]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query results:\n",
      "Result 1:\n",
      "Content: ['<p>And so it was indeed: she was now only ten inches high, and her face\\nbrightened up at the thought that she was now the right size for going\\nthrough the little door into that lovely garden. First, however, she\\nwaited for a few minutes to see if she was going to shrink any further:\\nshe felt a little nervous about this; “for it might end, you know,”\\nsaid Alice to herself, “in my going out altogether, like a candle. I\\nwonder what I should be like then?” And she tried to fancy what the', '<p>The first question of course was, how to get dry again: they had a\\nconsultation about this, and after a few minutes it seemed quite\\nnatural to Alice to find herself talking familiarly with them, as if\\nshe had known them all her life. Indeed, she had quite a long argument\\nwith the Lory, who at last turned sulky, and would only say, “I am\\nolder than you, and must know better;” and this Alice would not allow\\nwithout knowing how old it was, and, as the Lory positively refused to', 'good, that it made Alice quite hungry to look at them—“I wish they’d\\nget the trial done,” she thought, “and hand round the refreshments!”\\nBut there seemed to be no chance of this, so she began looking at\\neverything about her, to pass away the time.</p>']\n",
      "Metadata: [{'source': 'alice_in_wonderland.md', 'start_index': 10349}, {'source': 'alice_in_wonderland.md', 'start_index': 24638}, {'source': 'alice_in_wonderland.md', 'start_index': 129390}]\n",
      "ID: ['3c49a6fc-2848-457c-b88d-1fcdf0d29299', '702480ca-dc7d-4f7a-a201-86e0091bf294', '32a47f56-566d-4628-88c7-3517ad7cd4db']\n"
     ]
    }
   ],
   "source": [
    "# Example query\n",
    "query_text = \"Describe Alice's age and appearance in the story.\"\n",
    "query_embedding = embeddingModel.encode(query_text).tolist()\n",
    "\n",
    "# Perform the query\n",
    "query_results = collection.query(query_embeddings=[query_embedding], n_results=3)\n",
    "\n",
    "# Print the results\n",
    "print(\"Query results:\")\n",
    "for i, result in enumerate(query_results[\"documents\"]):\n",
    "    print(f\"Result {i+1}:\")\n",
    "    print(\"Content:\", result)\n",
    "    print(\"Metadata:\", query_results[\"metadatas\"][i])\n",
    "    print(\"ID:\", query_results[\"ids\"][i])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "# Define llm\n",
    "llm = Ollama(model=\"gemma2:2b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1y/yx271f4n4s52fqfr437r3qqh0000gn/T/ipykernel_9261/3336693696.py:21: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n",
      "  llm_chain = LLMChain(\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Adjusting the Prompt Template\n",
    "prompt = \"\"\"\n",
    "1. Use the following context to answer the question at the end.\n",
    "2. Be precise and avoid speculation. If the information isn't clear, say \"I don't know.\"\n",
    "3. Provide a concise, 2-3 sentence answer.\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Accurate Answer:\"\"\"\n",
    "\n",
    "\n",
    "QA_CHAIN_PROMPT = PromptTemplate.from_template(prompt)\n",
    "\n",
    "llm_chain = LLMChain(\n",
    "    llm=llm, \n",
    "    prompt=QA_CHAIN_PROMPT, \n",
    "    callbacks=None, \n",
    "    verbose=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1y/yx271f4n4s52fqfr437r3qqh0000gn/T/ipykernel_9261/1950838181.py:6: LangChainDeprecationWarning: This class is deprecated. Use the `create_stuff_documents_chain` constructor instead. See migration guide here: https://python.langchain.com/v0.2/docs/versions/migrating_chains/stuff_docs_chain/\n",
      "  combine_documents_chain = StuffDocumentsChain(\n"
     ]
    }
   ],
   "source": [
    "document_prompt = PromptTemplate(\n",
    "    input_variables=[\"page_content\", \"source\"],\n",
    "    template=\"Context:\\ncontent:{page_content}\\nsource:{source}\",\n",
    ")\n",
    "\n",
    "combine_documents_chain = StuffDocumentsChain(\n",
    "    llm_chain=llm_chain,\n",
    "    document_variable_name=\"context\",\n",
    "    document_prompt=document_prompt,\n",
    "    callbacks=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1y/yx271f4n4s52fqfr437r3qqh0000gn/T/ipykernel_9261/731285988.py:2: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
      "  embedding_model = HuggingFaceEmbeddings(model_name='all-MiniLM-L6-v2')\n",
      "/Users/aslan/Documents/dataScience/rag_langchain/myenv/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "embedding_model = HuggingFaceEmbeddings(model_name='all-MiniLM-L6-v2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "vector_store = Chroma.from_documents(\n",
    "    documents=chunks,  # This should be your list of Document objects\n",
    "    embedding=embedding_model,\n",
    "    persist_directory='./chromadb_data',\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain = RetrievalQA.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=vector_store.as_retriever(),  # Use the retriever from the vector store\n",
    "    verbose=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Generated Answer:\n",
      "When Alice fell into the rabbit hole, she went straight down and ended up in a deep well.  After falling, she got back on her feet and continued chasing the rabbit. \n",
      "\n",
      "Here's some more about Alice after falling: \n",
      "\n",
      "* **She was unharmed:** After falling, Alice didn't get hurt.\n",
      "* **Curious:** She found herself being curious about the rabbit.\n",
      "* **Chasing the Rabbit:**  Alice followed the rabbit into a large hole.\n",
      "* **She followed closely:** She ran to catch up with the rabbit and was right behind it. \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"What happened alice when she fall down to rabbit hole? Tell me something about Alice after her falling down\"\n",
    "\n",
    "# Run the query through the chain\n",
    "result = qa_chain.run(query)\n",
    "\n",
    "print(\"Generated Answer:\")\n",
    "print(result)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
